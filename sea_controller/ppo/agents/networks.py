import numpy as np
import tensorflow as tf


class MlpPolicy:
    def __init__(self, obs_dim, act_dim, hid1_mult, policy_logvar, obs_ph):
        self.obs_dim = obs_dim
        self.act_dim = act_dim
        self.hid1_mult = hid1_mult
        self.policy_logvar = policy_logvar
        self.obs_ph = obs_ph

    def build_network(self,):
        # hidden layer sizes determined by obs_dim and act_dim (hid2 is geometric mean)
        hid1_size = self.obs_dim * self.hid1_mult  # 10 empirically determined
        hid3_size = self.act_dim * 10  # 10 empirically determined
        hid2_size = int(np.sqrt(hid1_size * hid3_size))
        # heuristic to set learning rate based on NN size (tuned on 'Hopper-v1')
        self.lr = 9e-4 / np.sqrt(hid2_size)  # 9e-4 empirically determined
        # 3 hidden layers with tanh activations
        out = tf.compat.v1.layers.dense(self.obs_ph, hid1_size, tf.tanh,
                              kernel_initializer=tf.compat.v1.random_normal_initializer(
                                  stddev=np.sqrt(1 / self.obs_dim)), name="h1_actor")
        out = tf.compat.v1.layers.dense(out, hid2_size, tf.tanh,
                              kernel_initializer=tf.compat.v1.random_normal_initializer(
                                  stddev=np.sqrt(1 / hid1_size)), name="h2_actor")
        out = tf.compat.v1.layers.dense(out, hid3_size, tf.tanh,
                              kernel_initializer=tf.compat.v1.random_normal_initializer(
                                  stddev=np.sqrt(1 / hid2_size)), name="h3_actor")
        out = out*250
        self.means = tf.compat.v1.layers.dense(out, self.act_dim,
                                     kernel_initializer=tf.compat.v1.random_normal_initializer(
                                         stddev=np.sqrt(1 / hid3_size)), name="means")
        # logvar_speed is used to 'fool' gradient descent into making faster updates
        # to log-variances. heuristic sets logvar_speed based on network size.
        logvar_speed = (10 * hid3_size) // 48
        log_vars = tf.compat.v1.get_variable('logvars', (logvar_speed, self.act_dim), tf.float32,
                                   tf.compat.v1.constant_initializer(0.0))
        self.log_vars = tf.reduce_sum(input_tensor=log_vars, axis=0) + self.policy_logvar

        """ Sample from distribution, given observation """
        self.sampled_act = (self.means +
                            tf.exp(self.log_vars / 2.0) *
                            tf.random.normal(shape=(self.act_dim,)))

        print('Policy Params -- h1: {}, h2: {}, h3: {}, lr: {:.3g}, logvar_speed: {}'
              .format(hid1_size, hid2_size, hid3_size, self.lr, logvar_speed))

        return self.means, self.log_vars, self.lr

    def step(self, sess, obs):
        feed_dict = {self.obs_ph: obs}
        return sess.run(self.sampled_act, feed_dict=feed_dict)
